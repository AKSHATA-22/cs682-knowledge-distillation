{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linspace() received an invalid combination of arguments - got (int, int, float), but expected one of:\n * (Tensor start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Tensor start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f55305022e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHWGQQuantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/cs682/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs682/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MSCS/Fall 2024/CS682/Project/cs682-knowledge-distillation/HWGQQuantizer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute standard deviation of input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mq_levels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Quantization levels (simplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Apply quantization: map each value to the closest quantization level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linspace() received an invalid combination of arguments - got (int, int, float), but expected one of:\n * (Tensor start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Tensor start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "# from utils.HWGQQuantizer import HWGQQuantizer\n",
    "# import utils\n",
    "import torch\n",
    "\n",
    "from HWGQQuantizer import HWGQQuantizer\n",
    "weights = torch.rand(2,3,4)\n",
    "# print(weights)\n",
    "modle = HWGQQuantizer(bit_width=4)\n",
    "print(modle.quantize(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence Loss with logits: -3.787278652191162\n"
     ]
    }
   ],
   "source": [
    "from utils.layers import *\n",
    "\n",
    "student_output = torch.tensor([[2.0, 0.5, 1.0], [1.5, 1.2, 2.2]], requires_grad=True)\n",
    "combined_teacher_output = torch.tensor([[2.1, 0.4, 1.1], [1.6, 1.1, 2.3]], requires_grad=True)\n",
    "labels = torch.tensor([0, 2])  \n",
    "temperature = 2.0 \n",
    "\n",
    "loss = loss_kl_divergence_with_logits(student_output, labels, combined_teacher_output, temperature)\n",
    "\n",
    "print(\"KL Divergence Loss with logits:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, bit_width):\n",
    "        return x.clamp(-1, 1).mul(2**bit_width).round() / (2**bit_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward99(x, bit_width):\n",
    "    return 2*(torch.floor(((2**bit_width-1)/2)*(x+1))/(2**bit_width-1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(x, bit_width):\n",
    "    sigma = x.std()\n",
    "    q_levels = sigma * torch.linspace(0, 3, 2**bit_width)\n",
    "    # print(q_levels)\n",
    "    return q_levels[((x / sigma).unsqueeze(-1) >= q_levels[:-1]) & ((x / sigma).unsqueeze(-1) < q_levels[1:])].max(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward2(x, bit_width):\n",
    "    sigma = x.std()  # Standard deviation for scaling\n",
    "    return (sigma * torch.bucketize(x / sigma, torch.linspace(0, 3, 2**bit_width, device=x.device) , right=True) / (2**bit_width - 1)) * sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward3(x, bit_width):\n",
    "    sigma = x.std()  # Standard deviation for scaling\n",
    "    q_levels = sigma*torch.linspace(0, 3, 2**bit_width, device=x.device)  # Quantization levels\n",
    "    # print(sigma)\n",
    "    return q_levels[torch.bucketize(x.flatten(), q_levels, right=True) - 1].view_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_new_inline(x, bit_width):\n",
    "    sigma = x.std()\n",
    "    q_levels = torch.linspace(0, 3, 2**bit_width)\n",
    "    q_levels = q_levels.view(1, -1).expand_as(x)\n",
    "    # print()\n",
    "    # print(x >= (sigma * q_levels[:-1]))\n",
    "    # print(sigma * q_levels[1:])\n",
    "    # print(sigma * q_levels[:-1])\n",
    "    return torch.where(x >= sigma * q_levels[:-1], sigma * q_levels[1:], sigma * q_levels[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_new(x, bit_width):\n",
    "    sigma = x.std()  \n",
    "    q_levels = torch.linspace(0, 3, 2**bit_width) \n",
    "    # print(\"q.levels ki shape\", q_levels.shape)\n",
    "    quantized_x = torch.zeros_like(x)\n",
    "    for i, q in enumerate(q_levels):\n",
    "        mask = (x >= (sigma * q_levels[i - 1] if i > 0 else 0)) & (x < sigma * q)\n",
    "        quantized_x[mask] = sigma * q\n",
    "\n",
    "    return quantized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0307,  0.7787, -0.6081,  0.3215],\n",
      "        [ 0.3956, -0.0367, -0.2510,  0.2963],\n",
      "        [-0.9986,  0.8470,  0.5298, -0.6073]])\n",
      "tensor([[0.0000, 0.8141, 0.0000, 0.3489],\n",
      "        [0.4652, 0.0000, 0.0000, 0.3489],\n",
      "        [0.0000, 0.9304, 0.5815, 0.0000]])\n",
      "tensor([[-0.0667,  0.7333, -0.7333,  0.2000],\n",
      "        [ 0.3333, -0.0667, -0.3333,  0.2000],\n",
      "        [-1.0000,  0.7333,  0.4667, -0.7333]])\n",
      "Test failed: Outputs are different.\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 12 / 12 (100.0%)\n",
      "Greatest absolute difference: 1.0 at index (2, 0) (up to 0.0001 allowed)\n",
      "Greatest relative difference: inf at index (0, 0) (up to 0.0001 allowed)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(3,4) * 2 - 1  # Random input in range [-1, 1]\n",
    "bit_width = 4\n",
    "\n",
    "# Compute outputs\n",
    "print(x)\n",
    "out2 = forward_new(x, bit_width)\n",
    "out3 = forward99(x, bit_width)\n",
    "\n",
    "# out1 = forward_new_inline(x, bit_width)\n",
    "# print(out1)\n",
    "print(out2)\n",
    "print(out3)\n",
    "# Compare outputs\n",
    "try:\n",
    "    torch.testing.assert_close(out3, out2, rtol=1e-4, atol=1e-4)\n",
    "    print(\"Test passed: Outputs are the same.\")\n",
    "except AssertionError as e:\n",
    "    print(\"Test failed: Outputs are different.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3348, -0.9144,  0.6856, -0.9406],\n",
       "        [-0.0540, -0.0649,  0.8615, -0.2454],\n",
       "        [-0.4563, -0.3982,  0.9035,  0.0547]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
