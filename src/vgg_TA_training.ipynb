{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and tf.config.list_physical_devices('GPU'):\n",
    "    device = 'GPU'\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = 'CPU'\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import random\n",
    "\n",
    "# Path to the downloaded tgz file\n",
    "tgz_path = \"/home/asko/Documents/workspace/Fall-24/682/project/dataset/imagenette2.tgz\"\n",
    "extract_path = \"./imagenette\"  # Target folder for extraction\n",
    "\n",
    "# Extract the file\n",
    "with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "print(\"Extraction completed.\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),  # Resize images to a size suitable for VGG16\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as per VGG16\n",
    "])\n",
    "extract_path_train = \"./imagenette/imagenette2/train\"\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=extract_path_train,  # Imagenette URL\n",
    "    transform=transform\n",
    ")\n",
    "extract_path_val = \"./imagenette/imagenette2/val\"\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root=extract_path_val,  # Imagenette URL\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "sampler_train = sampler.SubsetRandomSampler(range(len(train_dataset)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler_train)\n",
    "\n",
    "batch_size = 16\n",
    "sampler_val = sampler.SubsetRandomSampler(range(1000))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=sampler_val)\n",
    "\n",
    "batch_size = 32\n",
    "sampler_test = sampler.SubsetRandomSampler(range(1000, len(val_dataset)), len(val_dataset)-1000)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=sampler_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_per_process_memory_fraction(0.95, device=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from TACNNModel.TACNN import TACNN\n",
    "\n",
    "alpha = 0.03\n",
    "temperature = 0.02\n",
    "\n",
    "# Load the VGG16 Teacher model\n",
    "import dill\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    teacher_model = dill.load(f)\n",
    "\n",
    "# Load the VGG16 model\n",
    "ta_model = TACNN(alpha=alpha, temperature=temperature, num_of_classes=10)\n",
    "\n",
    "device = 'cuda' if len(tf.config.list_physical_devices('GPU'))!=0 else 'cpu'\n",
    "teacher_model.to(device)\n",
    "ta_model.to(device)\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_outputs = []\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    teacher_outputs.append(teacher_model(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    print('Checking accuracy on validation set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = optim.Adam(ta_model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-5, momentum=0.98, nesterov=False)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 16  \n",
    "model_weights = None\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0 \n",
    "    # if model_weights is None:\n",
    "    #     model_weights = model.conv1_1.weight.clone()   \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        ta_model.train()           \n",
    "        \n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        ta_output = ta_model(images)\n",
    "        teacher_output = teacher_outputs[batch_idx]\n",
    "\n",
    "        loss = ta_model.risk(Y=labels, teacher_preds=teacher_output, output=ta_output)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # total_loss += loss.item()\n",
    "        if batch_idx % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (batch_idx, loss.item()))\n",
    "                check_accuracy_part34(val_loader, ta_model)\n",
    "                print()\n",
    "    print(f\"Epoch {epoch} complete\")\n",
    "    # print(torch.sum(model_weights - model.conv1_1.weight))   \n",
    "    # model_weights = model.conv1_1.weight.clone()        \n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / (batch_idx+1)}, Training Accuracy: {check_accuracy_part34(train_loader, model)}\")\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / (batch_idx+1)}, Validation Accuracy: {check_accuracy_part34(val_loader, model)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs682",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bccbcb84f00369f66728157738030ca7cc2ef9bb77b354aa1c094be760858386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
