{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and tf.config.list_physical_devices('GPU'):\n",
    "    device = 'GPU'\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = 'CPU'\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asko/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imagenet_resized',\n",
      "    full_name='imagenet_resized/64x64/0.1.0',\n",
      "    description=\"\"\"\n",
      "    This dataset consists of the ImageNet dataset resized to fixed size. The images\n",
      "    here are the ones provided by Chrabaszcz et. al. using the box resize method.\n",
      "    \n",
      "    For [downsampled ImageNet](http://image-net.org/download.php) for unsupervised\n",
      "    learning see `downsampled_imagenet`.\n",
      "    \n",
      "    WARNING: The integer labels used are defined by the authors and do not match\n",
      "    those from the other ImageNet datasets provided by Tensorflow datasets. See the\n",
      "    original\n",
      "    [label list](https://github.com/PatrykChrabaszcz/Imagenet32_Scripts/blob/master/map_clsloc.txt),\n",
      "    and the\n",
      "    [labels used by this dataset](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image_classification/imagenet_resized_labels.txt).\n",
      "    Additionally, the original authors 1 index there labels which we convert to 0\n",
      "    indexed by subtracting one.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Images resized to 64x64\n",
      "    \"\"\",\n",
      "    homepage='https://patrykchrabaszcz.github.io/Imagenet32/',\n",
      "    data_path='/home/asko/tensorflow_datasets/downloads/manual/imagenet_resized/64x64/0.1.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=13.13 GiB,\n",
      "    dataset_size=10.29 GiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(64, 64, 3), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=1000),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'train': <SplitInfo num_examples=1281167, num_shards=128>,\n",
      "        'validation': <SplitInfo num_examples=50000, num_shards=4>,\n",
      "    },\n",
      "    citation=\"\"\"@article{chrabaszcz2017downsampled,\n",
      "      title={A downsampled variant of imagenet as an alternative to the cifar datasets},\n",
      "      author={Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},\n",
      "      journal={arXiv preprint arXiv:1707.08819},\n",
      "      year={2017}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:48:19.540097: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 18:48:19.541578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-15 18:48:19.541893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-15 18:48:19.542032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-15 18:48:19.587927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-15 18:48:19.588055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-15 18:48:19.588131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-15 18:48:19.588217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4309 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the ImageNet dataset\n",
    "\n",
    "data_dir = '/home/asko/tensorflow_datasets/downloads/manual'\n",
    "\n",
    "# Load the ImageNet Resized dataset (e.g., 32x32)\n",
    "dataset, info = tfds.load('imagenet_resized/64x64', data_dir=data_dir, with_info=True, as_supervised=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "# Print the dataset info\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications import VGG16\n",
    "from VGG16.vgg import VGG\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "alpha = 0.03\n",
    "temperature = 0.02\n",
    "\n",
    "# Load the VGG16 model\n",
    "model = VGG(alpha=alpha, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "data_dir = '/home/asko/tensorflow_datasets/downloads/manual'\n",
    "dataset, info = tfds.load('imagenet_resized/64x64', data_dir=data_dir, with_info=True, as_supervised=True)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, [64, 64])\n",
    "    image = preprocess_input(image) \n",
    "    return image, label\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = (train_dataset\n",
    "                 .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                 .shuffle(1000)  \n",
    "                 .batch(batch_size)\n",
    "                 .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_dataset = (val_dataset\n",
    "               .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "               .batch(batch_size)\n",
    "               .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Path to the downloaded tgz file\n",
    "tgz_path = \"/Users/apple/imagenette2-320.tgz\"\n",
    "extract_path = \"./imagenette\"  # Target folder for extraction\n",
    "\n",
    "# Extract the file\n",
    "with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "print(\"Extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "data_dir = '/Users/apple/Documents/MSCS/Fall 2024/CS682/Project/cs682-knowledge-distillation/imagenette2-320.tgz'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227,227)),  # Resize images to a size suitable for VGG16\n",
    "    # transforms.CenterCrop(224),  # Crop to the expected input size for VGG16\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as per VGG16\n",
    "])\n",
    "extract_path = \"./imagenette\"\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=extract_path,  # Imagenette URL\n",
    "    transform=transform\n",
    ")\n",
    "# subset_indices = list(range(16000))  \n",
    "# train_subset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "batch_size = 50\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "    # print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([50, 3, 227, 227])\n",
      "tensor([[[[-1.5528, -1.5699, -1.5699,  ..., -1.0733, -1.1075, -1.1418],\n",
      "          [-1.5699, -1.5699, -1.5699,  ..., -1.0733, -1.0904, -1.1418],\n",
      "          [-1.5699, -1.5699, -1.5699,  ..., -1.0733, -1.0904, -1.1247],\n",
      "          ...,\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.6727, -1.5357, -1.6898],\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.6727, -1.5357, -1.6898],\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.6727, -1.5357, -1.6898]],\n",
      "\n",
      "         [[-1.1954, -1.1779, -1.1779,  ..., -0.2325, -0.2675, -0.3025],\n",
      "          [-1.1779, -1.1779, -1.1779,  ..., -0.2325, -0.2500, -0.3025],\n",
      "          [-1.1779, -1.1779, -1.1779,  ..., -0.2500, -0.2675, -0.3025],\n",
      "          ...,\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -1.5805, -1.4405, -1.5980],\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -1.5805, -1.4405, -1.5980],\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -1.5805, -1.4405, -1.5980]],\n",
      "\n",
      "         [[-0.4624, -0.4624, -0.4624,  ...,  1.0365,  1.0017,  0.9668],\n",
      "          [-0.4624, -0.4624, -0.4624,  ...,  1.0365,  1.0191,  0.9668],\n",
      "          [-0.4624, -0.4624, -0.4624,  ...,  1.0539,  1.0365,  1.0017],\n",
      "          ...,\n",
      "          [-1.2990, -1.2990, -1.2990,  ..., -1.3861, -1.2467, -1.4036],\n",
      "          [-1.2990, -1.2990, -1.2990,  ..., -1.3861, -1.2467, -1.4036],\n",
      "          [-1.2990, -1.2990, -1.2990,  ..., -1.3861, -1.2467, -1.4036]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1872,  1.1187,  1.1015,  ...,  1.0673,  1.0673,  1.0673],\n",
      "          [ 1.0331,  1.0331,  0.9646,  ...,  1.0844,  1.0844,  1.0844],\n",
      "          [ 0.9132,  0.9474,  0.8104,  ...,  1.0844,  1.0844,  1.0844],\n",
      "          ...,\n",
      "          [ 0.5536,  0.4337,  0.2282,  ...,  0.2453,  0.0912,  0.0741],\n",
      "          [ 0.5364,  0.4166,  0.2796,  ...,  0.2796,  0.0912,  0.1254],\n",
      "          [ 0.4337,  0.2796,  0.3138,  ...,  0.3481, -0.0458,  0.1597]],\n",
      "\n",
      "         [[ 1.6232,  1.5357,  1.5357,  ...,  1.5532,  1.5532,  1.5532],\n",
      "          [ 1.4482,  1.4657,  1.4132,  ...,  1.5357,  1.5357,  1.5357],\n",
      "          [ 1.3081,  1.3782,  1.2556,  ...,  1.5357,  1.5357,  1.5357],\n",
      "          ...,\n",
      "          [ 0.7654,  0.6254,  0.3978,  ...,  0.4328,  0.2577,  0.2402],\n",
      "          [ 0.7654,  0.6429,  0.4678,  ...,  0.4328,  0.2752,  0.3277],\n",
      "          [ 0.6779,  0.4853,  0.4853,  ...,  0.4853,  0.1352,  0.3627]],\n",
      "\n",
      "         [[ 2.1171,  2.0648,  2.0997,  ...,  2.0474,  2.0300,  2.0125],\n",
      "          [ 1.9428,  1.9777,  1.9777,  ...,  2.0648,  2.0474,  2.0300],\n",
      "          [ 1.7685,  1.8731,  1.8034,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          ...,\n",
      "          [ 0.9842,  0.8448,  0.6182,  ...,  0.6008,  0.4439,  0.4265],\n",
      "          [ 1.0017,  0.8622,  0.7054,  ...,  0.7402,  0.6182,  0.6531],\n",
      "          [ 0.8971,  0.7228,  0.7228,  ...,  0.8797,  0.5485,  0.7751]]],\n",
      "\n",
      "\n",
      "        [[[-2.0323, -2.0665, -2.0837,  ..., -1.8953, -1.8953, -1.8953],\n",
      "          [-2.0323, -2.0665, -2.0837,  ..., -1.8610, -1.8610, -1.8268],\n",
      "          [-1.9295, -1.9980, -2.0837,  ..., -1.7925, -1.7925, -1.7925],\n",
      "          ...,\n",
      "          [-1.1247, -1.1589, -1.1247,  ..., -1.0219, -0.9534, -1.1760],\n",
      "          [-1.0390, -1.0904, -1.0562,  ..., -1.0048, -0.9705, -1.0904],\n",
      "          [-1.0048, -1.0219, -1.1418,  ..., -1.1418, -1.0733, -1.1247]],\n",
      "\n",
      "         [[-1.9482, -1.9832, -2.0007,  ..., -1.6856, -1.6856, -1.6856],\n",
      "          [-1.9482, -1.9832, -2.0007,  ..., -1.6506, -1.6506, -1.6155],\n",
      "          [-1.8431, -1.9132, -2.0007,  ..., -1.6331, -1.6331, -1.6331],\n",
      "          ...,\n",
      "          [-1.0903, -1.1253, -1.0903,  ..., -0.9328, -0.8627, -1.0903],\n",
      "          [-1.0028, -1.0553, -1.0203,  ..., -0.9153, -0.8803, -1.0028],\n",
      "          [-0.9678, -0.9853, -1.1078,  ..., -1.0553, -0.9853, -1.0378]],\n",
      "\n",
      "         [[-1.7522, -1.7870, -1.8044,  ..., -1.3164, -1.3164, -1.3164],\n",
      "          [-1.7522, -1.7870, -1.8044,  ..., -1.2816, -1.2816, -1.2467],\n",
      "          [-1.6476, -1.7173, -1.7870,  ..., -1.2467, -1.2467, -1.2467],\n",
      "          ...,\n",
      "          [-0.9156, -0.9504, -0.9156,  ..., -0.7413, -0.6715, -0.8981],\n",
      "          [-0.8284, -0.8807, -0.8458,  ..., -0.7238, -0.6890, -0.8110],\n",
      "          [-0.7936, -0.8110, -0.9330,  ..., -0.8633, -0.7936, -0.8458]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9534, -0.9363, -0.9192,  ..., -1.4843, -1.3815, -1.3815],\n",
      "          [-0.9534, -0.9363, -0.9020,  ..., -1.4843, -1.3644, -1.3815],\n",
      "          [-0.9534, -0.9192, -0.8849,  ..., -1.4672, -1.3644, -1.3815],\n",
      "          ...,\n",
      "          [-2.0837, -2.0837, -2.0323,  ..., -0.0972, -0.2513, -0.1828],\n",
      "          [-2.0323, -2.0665, -2.0323,  ...,  0.0741, -0.4568, -0.5767],\n",
      "          [-1.9980, -2.0323, -2.0152,  ..., -0.0629, -0.4397, -0.7308]],\n",
      "\n",
      "         [[-0.6702, -0.6527, -0.6352,  ..., -1.2304, -1.1253, -1.0903],\n",
      "          [-0.6702, -0.6527, -0.6176,  ..., -1.2304, -1.1078, -1.0903],\n",
      "          [-0.6702, -0.6352, -0.6001,  ..., -1.2129, -1.1078, -1.0903],\n",
      "          ...,\n",
      "          [-1.9132, -1.9132, -1.8606,  ...,  0.5203,  0.4153,  0.5203],\n",
      "          [-1.8606, -1.8957, -1.8606,  ...,  0.6954,  0.1877,  0.1352],\n",
      "          [-1.8256, -1.8606, -1.8431,  ...,  0.5378,  0.2227, -0.0224]],\n",
      "\n",
      "         [[-0.2881, -0.2707, -0.2532,  ..., -0.9330, -0.7936, -0.7587],\n",
      "          [-0.2881, -0.2707, -0.2358,  ..., -0.9330, -0.7936, -0.7587],\n",
      "          [-0.2881, -0.2532, -0.2184,  ..., -0.9156, -0.7936, -0.7587],\n",
      "          ...,\n",
      "          [-1.7522, -1.7522, -1.6999,  ..., -0.1312, -0.3927, -0.3578],\n",
      "          [-1.6999, -1.7347, -1.6999,  ...,  0.0082, -0.6193, -0.7587],\n",
      "          [-1.6650, -1.6999, -1.6824,  ..., -0.1661, -0.6018, -0.9156]]],\n",
      "\n",
      "\n",
      "        [[[-1.0048, -0.8335, -0.9020,  ..., -0.9020, -1.1075, -1.2959],\n",
      "          [-1.1932, -1.0219, -0.9877,  ..., -0.8678, -0.9705, -1.1418],\n",
      "          [-1.1760, -1.0904, -0.9877,  ..., -1.0562, -1.1075, -0.9705],\n",
      "          ...,\n",
      "          [ 0.3309,  0.1768,  0.4166,  ..., -0.4568, -0.5082, -0.4739],\n",
      "          [ 0.3481,  0.4679,  0.3138,  ..., -0.2856, -0.2684, -0.0116],\n",
      "          [ 0.5364,  0.5878,  0.3652,  ..., -0.0972, -0.3198, -0.0801]],\n",
      "\n",
      "         [[-0.9678, -0.7927, -0.8627,  ..., -1.0728, -1.2654, -1.4930],\n",
      "          [-1.1429, -0.9678, -0.9328,  ..., -0.9853, -1.1078, -1.3004],\n",
      "          [-1.1078, -1.0028, -0.8978,  ..., -1.1078, -1.1779, -1.0553],\n",
      "          ...,\n",
      "          [ 0.1877,  0.0301,  0.2752,  ..., -0.6527, -0.6702, -0.5826],\n",
      "          [ 0.2052,  0.3277,  0.1702,  ..., -0.4951, -0.4251, -0.1625],\n",
      "          [ 0.3978,  0.4503,  0.2227,  ..., -0.3025, -0.4951, -0.2150]],\n",
      "\n",
      "         [[-0.9504, -0.7761, -0.8458,  ..., -0.9678, -1.1596, -1.3687],\n",
      "          [-1.1421, -0.9678, -0.9330,  ..., -0.8981, -1.0201, -1.1944],\n",
      "          [-1.1073, -1.0027, -0.8981,  ..., -1.0550, -1.1073, -1.0027],\n",
      "          ...,\n",
      "          [ 0.4265,  0.2696,  0.5136,  ..., -0.5321, -0.5844, -0.5147],\n",
      "          [ 0.4439,  0.5659,  0.3916,  ..., -0.3927, -0.3404, -0.0615],\n",
      "          [ 0.6356,  0.6879,  0.4439,  ..., -0.2010, -0.3927, -0.1138]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2214,  1.2899,  1.3584,  ...,  0.7762,  0.7591,  0.7591],\n",
      "          [ 1.2728,  1.3242,  1.3584,  ...,  0.6734,  0.6734,  0.6563],\n",
      "          [ 1.2557,  1.2728,  1.2728,  ...,  0.5707,  0.5707,  0.5707],\n",
      "          ...,\n",
      "          [-0.5767, -0.8335, -0.6965,  ...,  0.4166,  1.0502,  0.7591],\n",
      "          [-0.4397, -0.4739, -0.2684,  ...,  0.5707,  1.0844,  1.4954],\n",
      "          [-0.0629, -0.0116, -0.2171,  ...,  0.3652,  0.9474,  1.6324]],\n",
      "\n",
      "         [[ 1.5357,  1.6057,  1.6758,  ...,  1.3957,  1.3782,  1.3782],\n",
      "          [ 1.5882,  1.6408,  1.6758,  ...,  1.3431,  1.3431,  1.3256],\n",
      "          [ 1.6057,  1.6232,  1.6408,  ...,  1.2556,  1.2731,  1.2731],\n",
      "          ...,\n",
      "          [-0.9853, -1.1779, -0.9678,  ..., -0.0574,  0.6779,  0.5553],\n",
      "          [-0.8978, -0.8803, -0.5651,  ...,  0.0126,  0.5553,  1.1331],\n",
      "          [-0.5476, -0.4251, -0.5476,  ..., -0.2325,  0.3277,  1.1681]],\n",
      "\n",
      "         [[ 2.0125,  2.0823,  2.1520,  ...,  1.9603,  1.9428,  1.9428],\n",
      "          [ 2.0648,  2.1171,  2.1520,  ...,  1.9254,  1.9080,  1.8905],\n",
      "          [ 2.0648,  2.0823,  2.0997,  ...,  1.8383,  1.8557,  1.8557],\n",
      "          ...,\n",
      "          [-1.1247, -1.2990, -1.0550,  ..., -0.0441,  0.7054,  0.6008],\n",
      "          [-1.0898, -1.0376, -0.7064,  ...,  0.0779,  0.6356,  1.2457],\n",
      "          [-0.7761, -0.6193, -0.7238,  ..., -0.1487,  0.4614,  1.3328]]]])\n",
      "Batch of labels shape: torch.Size([50])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")  # Shape: [batch_size, 3, 64, 64]\n",
    "    print(images)\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")  # Shape: [batch_size]\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from VGG16.vgg import VGG\n",
    "\n",
    "alpha = 0.03\n",
    "temperature = 0.02\n",
    "\n",
    "# Load the VGG16 model\n",
    "model = VGG(alpha=alpha, temperature=temperature)\n",
    "\n",
    "device = 'cuda' if len(tf.config.list_physical_devices('GPU'))!=0 else 'cpu'\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    print(device)\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            print(y)\n",
    "            print(preds)\n",
    "            # print(scores)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        # print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return 100 * acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part35(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # Move inputs to device\n",
    "            y = y.to(device=device, dtype=torch.long)  # Move labels to device, long for classification\n",
    "            \n",
    "            scores = model(x)  # Get model predictions\n",
    "            _, preds = scores.max(1)  # Get predicted class indices\n",
    "            \n",
    "            num_correct += preds.eq(y).sum().item()  # Compare predictions with ground truth\n",
    "            num_samples += preds.size(0)  # Count samples\n",
    "\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return 100 * acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            if t % 100 == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                # check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes=1000):\n",
    "    # Initialize a tensor of zeros with the shape [batch_size, num_classes]\n",
    "    one_hot_labels = torch.zeros(labels.size(0), num_classes).cuda()\n",
    "    one_hot_labels.scatter_(1, labels.view(-1, 1), 1)  # Set 1 at the correct index\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Got 1300 / 16000 correct (8.12)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy_part34(train_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs682",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "bccbcb84f00369f66728157738030ca7cc2ef9bb77b354aa1c094be760858386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
